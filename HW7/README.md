# 作业1.7
## CartPole
### 说明：
> cartpole.py中实现了DQN算法（with replay buffer），要求看懂代码并尝试调参（可以改任何你想改的部分），撰写实验报告（1-2页为宜）。
### 做法：
可调节的超参数有：
1. 网络结构
    > 层数<br>
    > 每层宽度<br>
    > （激活函数）<br>
    > （优化器）
2. 常数参数
    > γ, ε<br>
    > （batch_size, buffer_size, update_freq）
## Bonus：Maze
### 说明
> maze.py中实现了迷宫环境。要求使用基于神经网络的值迭代算法（DQN/SARSA等）求解迷宫，并用print_maze_policy输出学习得到的策略。<br>
> 可以参考CartPole的DQN实现来完成此任务。<br>
> 按时正确实现总评+1分。不完成此任务不会扣分。
### 做法
使用DQN算法
> 初始阶段探索很慢，前几个episode要几万步。
>
> 两分钟后网络有了一些效果，平均在两三百步。但其实离通路还差很远，因为最佳策略是16步，一百步已经走遍了整个迷宫了，应该只是它学习到了某个局部怎么走。
>
> 大概十分钟左右，可能是突然有几次随机化的探索和更新，步数迅速下降，然后达到了最优策略。随后在这个最优策略下快速迭代得到结果。
>
> 它基本给出了最优路径，但在非最优路径上的节点的策略不是最佳的，因为环境总是从某个确定的起点开始，所以对于那些难以走到的节点不怎么去更新。
>
> 注意到它在一些靠墙的位置上它会选择撞墙，可能是因为网络表达能力不够的问题？

### 问题
1. 奖励的设计
    1. Rstep = -1, Rend = 0, γ = 1: 一个步数很大的episode影响比，震荡太大。
    2. Rstep = -1, Rend = 0, γ < 1: 步数足够大时，奖励依等比数列求和收敛，路径的长短没有显著区别，早期难以优化。
    3. Rstep = 0, Rend > 0, γ = 1: 路径长短没有区别，无法优化。
    4. Rstep = 0, Rend > 0, γ < 1: 效果不错。
    5. Restp = -1, Rend > 0, γ < 1: 效果不错。
2. 收敛性

    我发现了一种偶然的情况：大约五百个episode就发现了最优路径，但突然步数剧增，并且最终给出了错误的策略。

    我的分析如下：
    > 这个策略的产生是某次网络的更新冲到了一个差的位置。神经网络不是连续的映射，可能权值上微小的改动就会造成策略的剧变，它给出一个环路或者撞墙的策略，并且错误范围很大。
    >
    > 由于是在训练的后期，epsilon已经很小，探索性不足，需要几万步才能打破策略的概率到达终点。而buffer size只设置了一万，故样本池被这些无用样本占满了，很难再学习到有效动作。
    >
    > 概括来说就是陷入了壁垒非常高的局部极小值
