# 作业1.5
## Windy Gridworld
### 说明
> Sutton RL book P130 Example 6.5<br>
> 分别实现SARSA算法和Q-Learning算法（仅需在代码//TODO处修改即可），输出最优路径和步数（已实现）。
### 提示
1. 答案参考P130页图，若与图不同，到达目标的步数应不超过17。
2. 不用调整//TODO处之外的部分。
### 做法
```txt
每次step对应1 iter（iter指时间步）
每个episode是一条路径：1初始化、2状态转移、3终止
1. 本问题中只需初始化为起点(0,3)，对于某些起点也是决策的问题则可能要遍历初始化
2. TD方法中状态转移是序贯的，每一步决策动作时给出下一个状态。这一阶段还要更新Q(s,a)
3. 当s为终点时终止

我最初在learn函数中【直接递归】模拟episode：未done就递归继续下一步，done就递归新建环境
这样递归没有及时清除已结束的episode，迭代数万次就会【爆栈】
迭代必须按episode（调用）或者step（循环）分解

SARSA和Q-Learning的区别：
更新价值时，前者的a'是策略生成的，后者则取最大值
序贯状态转移时，前者同时转移s'和a'，后者只转移s'
```