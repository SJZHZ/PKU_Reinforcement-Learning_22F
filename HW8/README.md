# 作业1.8 策略梯度算法
## CartPole
### 说明
> 使用策略梯度算法求解CartPole环境，撰写实验报告（1页A4）
### 提示
1. 可以考虑使用REINFORCE算法求解，episode结束时给予-1的reward。
2. 鼓励独立完成。使用Pytorch不熟练的同学可以参考网上代码实现，但需要理解后借鉴。
### 做法
[REINFORCE算法参考代码](https://www.cnblogs.com/pkgunboat/p/14585134.html)
1. 模型并不是以往的价值估计V(S, A)，而是概率估计P(S,A)
2. REINFORCE算法是基于蒙特卡洛的。在一个episode之后逆路径更新，每一步要乘以概率权值。
    ```python
    for i in reversed(range(len(rewards))):     # 逆路径计算
        R = rewards[i] + self.GAMMA * R
        loss = loss - R * log_probs[i]          # 概率权重
    ```
3. 通过策略的梯度迭代，可以逐渐找到价值更高的策略，从而取得最优策略
4. 因为是基于蒙特卡洛方法的，方差较大，收敛较慢
5. 环境的宽松问题：
    > 奖励的上限只有500，无法区分能做到500以上的策略。
    >
    > 一个理论最优策略未必要时刻维持平衡。它可以在早期随意操作，直到后期快倒了才开始努力抢救，依然可以回正。
    >
    > 但策略模型只是一个近似模拟，它的动作不是决定性的，而是概率选择的。在边缘玩火还是有概率玩脱，所以输出这样的策略对它来说不是一个最优解（最优解是时刻维持平衡）。
    >
    > 当模型坚持到500时，它不会发现自己不够完美。只有当小概率情况它玩脱了的时候，它才会进行更新。并且早期策略比较随意，概率分散。反而是后期策略由于要尽力抢救，确定性高。所以真正需要更新的早期策略学习缓慢，而且每次还伴随着后期策略的少量震荡。